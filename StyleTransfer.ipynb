{"cells":[{"cell_type":"markdown","metadata":{"id":"PTI6cfKdIEE2"},"source":["https://github.com/anh-nn01/Neural-Style-Transfer"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1H3kGZTpx0WyWEfsFkh5_sqOkUZ5xQQqI"},"executionInfo":{"elapsed":253046,"status":"ok","timestamp":1649142014857,"user":{"displayName":"Devesh D.R","userId":"03004794869479780151"},"user_tz":-330},"id":"ce0-dZHoFmSx","outputId":"3869bf78-b204-496a-9f39-2c4b5769f07e"},"outputs":[],"source":["import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from PIL import Image\n","import imageio\n","import cv2\n","# import png\n","\n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","\n","if gpus:\n","    try:\n","        for gpu in gpus:\n","            tf.config.experimental.set_memory_growth(gpu, True)\n","    except RuntimeError as e:\n","        print(e)\n","\n","\n","\n","\"\"\"\n","Return the activations in different convolutional layers in VGG19 of img\n","\"\"\"\n","def model_activations(img, input):\n","\n","    model = tf.keras.applications.VGG19(include_top = False, weights = \"imagenet\")\n","    model.trainable = False # we will not change the paramenters of the VGG19\n","    # model.summary()\n","\n","    res = {}\n","\n","    \"\"\"\n","    Define layers of VGG19 from Tensorflow\n","    \"\"\"\n","    input = model.get_layer(name = input)(img)\n","\n","    block1_conv1_features = model.get_layer(name = \"block1_conv1\")(input)\n","    block1_conv2_features = model.get_layer(name = \"block1_conv2\")(block1_conv1_features)\n","    block1_pool_features  = model.get_layer(name = \"block1_pool\")(block1_conv2_features)\n","\n","    block2_conv1_features = model.get_layer(name = \"block2_conv1\")(block1_pool_features)\n","    block2_conv2_features = model.get_layer(name = \"block2_conv2\")(block2_conv1_features)\n","    block2_pool_features  = model.get_layer(name = \"block2_pool\")(block2_conv2_features)\n","\n","    block3_conv1_features = model.get_layer(name = \"block3_conv1\")(block2_pool_features)\n","    block3_conv2_features = model.get_layer(name = \"block3_conv2\")(block3_conv1_features)\n","    block3_conv3_features = model.get_layer(name = \"block3_conv3\")(block3_conv2_features)\n","    block3_conv4_features = model.get_layer(name = \"block3_conv4\")(block3_conv3_features)\n","    block3_pool_features  = model.get_layer(name = \"block3_pool\")(block3_conv4_features)\n","\n","    block4_conv1_features = model.get_layer(name = \"block4_conv1\")(block3_pool_features)\n","    block4_conv2_features = model.get_layer(name = \"block4_conv2\")(block4_conv1_features)\n","    block4_conv3_features = model.get_layer(name = \"block4_conv3\")(block4_conv2_features)\n","    block4_conv4_features = model.get_layer(name = \"block4_conv4\")(block4_conv3_features)\n","    block4_pool_features = model.get_layer(name  = \"block4_pool\")(block4_conv4_features)\n","\n","    block5_conv1_features = model.get_layer(name = \"block5_conv1\")(block4_conv4_features)\n","    block5_conv2_features = model.get_layer(name = \"block5_conv2\")(block5_conv1_features)\n","    block5_conv3_features = model.get_layer(name = \"block5_conv3\")(block5_conv2_features)\n","    block5_conv4_features = model.get_layer(name = \"block5_conv4\")(block5_conv3_features)\n","    block5_pool_features  = model.get_layer(name = \"block5_pool\")(block5_conv4_features)\n","\n","    res[\"b1_conv1_activation\"] = block1_conv1_features\n","    res[\"b1_conv2_activation\"] = block1_conv2_features\n","    res[\"b1_pool_activation\"]  = block1_pool_features\n","\n","    res[\"b2_conv1_activation\"] = block2_conv1_features\n","    res[\"b2_conv2_activation\"] = block2_conv2_features\n","    res[\"b2_pool_activation\"]  = block2_pool_features\n","\n","    res[\"b3_conv1_activation\"] = block3_conv1_features\n","    res[\"b3_conv2_activation\"] = block3_conv2_features\n","    res[\"b3_conv3_activation\"] = block3_conv3_features\n","    res[\"b3_conv4_activation\"] = block3_conv4_features\n","    res[\"b3_pool_activation\"]  = block3_pool_features\n","\n","    res[\"b4_conv1_activation\"] = block4_conv1_features\n","    res[\"b4_conv2_activation\"] = block4_conv2_features\n","    res[\"b4_conv3_activation\"] = block4_conv3_features\n","    res[\"b4_conv4_activation\"] = block4_conv4_features\n","    res[\"b4_pool_activation\"]  = block4_pool_features\n","\n","    res[\"b5_conv1_activation\"] = block5_conv1_features\n","    res[\"b5_conv2_activation\"] = block5_conv2_features\n","    res[\"b5_conv3_activation\"] = block5_conv3_features\n","    res[\"b5_conv4_activation\"] = block5_conv4_features\n","    res[\"b5_pool_activation\"]  = block5_pool_features\n","\n","    return res\n","\n","\"\"\"\n","Define the Content Loss / Texture Loss\n","@param activation_product: activations of the Generated Image at a given convolutional layer\n","@param activation_content: activations of the Content Image at the corresponding convolutional layer \n","\"\"\"\n","def Loss_C(activation_product, activation_content):\n","    # index 0 is the number of images by default\n","    n_H = activation_product.shape[1] # vertical dimension of a channel in current activation layer\n","    n_W = activation_product.shape[2] # horizontal dimension of a channel in current activation layer\n","    n_C = activation_product.shape[3] # number of channels in current activation layer\n","\n","    loss = tf.reduce_sum(tf.pow(activation_product - activation_content, 2))\n","    loss = (1 / (4 * n_H * n_W * n_C)) * loss\n","\n","    return loss\n","\n","\"\"\"\n","Define the Perceptual Loss / Feature Loss for 1 layer\n","The core of the algorithm\n","@param activation_product: activations of the Generated Image at a given convolutional layer\n","@param activation_perceptual: activations of the Style Image at the corresponding convolutional layer\n","\"\"\"\n","def Loss_P_layer(activation_product, activation_perceptual):\n","    # index 0 is the number of images by default\n","    n_H = activation_product.shape[1] # vertical dimension of a channel in current activation layer\n","    n_W = activation_product.shape[2] # horizontal dimension of a channel in current activation layer\n","    n_C = activation_product.shape[3] # number of channels in current activation layer\n","\n","    # The shape of the activation layer in the CNN\n","    # Both activation_product and activation_perceptual must have the same dimension (bc of the same corresponding layer)\n","    layer_shape = activation_product.shape\n","\n","    # unroll matrix for computational efficiency, I will explain it more clearly in the notebook / github\n","    # Note: Use tf.reshape() instead of np.reshape()\n","    unroll_product = tf.reshape(activation_product, (1, layer_shape[1] * layer_shape[2], layer_shape[3]))\n","    unroll_perceptual = tf.reshape(activation_perceptual, (1, layer_shape[1] * layer_shape[2], layer_shape[3]))\n","\n","    # Define Gram Matrices\n","    # Note: Core of the the entire Algorithm\n","    G_product = tf.matmul(tf.transpose(unroll_product[0]), unroll_product[0])\n","    G_perceptual = tf.matmul(tf.transpose(unroll_perceptual[0]), unroll_perceptual[0])\n","\n","    Loss = tf.reduce_sum(tf.pow(G_product - G_perceptual, 2))\n","    Loss = (1 / (2 * n_H * n_W * n_C)**2) * Loss\n","\n","    return Loss \n","\n","\n","\n","\n","\"\"\"\n","Draw the picture\n","\"\"\"\n","def Draw(epoch, opt):\n","  layer_1_C = 'b4_conv2_activation'\n","\n","  layer_1_P = 'b1_conv1_activation'\n","  layer_2_P = 'b2_conv1_activation'\n","  layer_3_P = 'b3_conv1_activation'\n","  layer_4_P = 'b4_conv1_activation'\n","  layer_5_P = 'b5_conv1_activation'\n","\n","  with tf.GradientTape() as tape:\n","      # Layer activations of the product through VGG19\n","      product_activation = model_activations(product, input = \"input_\" + str(epoch+3))\n","\n","      # Define Loss function / Optimization goal\n","      loss_C = Loss_C(product_activation[layer_1_C], activation_C[layer_1_C])\n","      loss_P = 0.05 * Loss_P_layer(product_activation[layer_1_P], activation_P[layer_1_P]) + 0.05 * Loss_P_layer(product_activation[layer_2_P], activation_P[layer_2_P]) + 0.1 * Loss_P_layer(product_activation[layer_3_P], activation_P[layer_3_P]) + 0.3 * Loss_P_layer(product_activation[layer_4_P], activation_P[layer_4_P]) + 0.7 * Loss_P_layer(product_activation[layer_5_P], activation_P[layer_5_P])\n","      loss = gamma_1 * loss_C + gamma_2 * loss_P\n","      \n","      # Compute Gradient / Direction with highest rate of change\n","      grad = tape.gradient(loss, product)\n","      # Apply Gradient on the pixels\n","      opt.apply_gradients([(grad, product)])\n","\n","      # Clip the pixel values that fall outside the range of [0,1]\n","      product.assign(tf.clip_by_value(product, clip_value_min=0.0, clip_value_max=1.0))\n","\n","      \n","      #show resulting image after each epoch\n","      plt.imshow(product[0,:,:,:])\n","      plt.title(\"Drawing... Epoch \" + str(epoch))\n","      plt.show()\n","      print(loss)\n","      \n","\n","\"\"\"\n","Create an empty template to start drawing\n","\"\"\"\n","def create_template():\n","    img_temp = np.ones((image_shape[0], image_shape[1], image_shape[2])) * 255\n","    blank_paper = Image.fromarray(img_temp.astype('uint8')).convert('RGB')\n","    blank_paper = tf.keras.preprocessing.image.img_to_array(blank_paper)\n","    blank_paper = np.expand_dims(blank_paper, axis = 0)\n","    blank_paper = tf.Variable(blank_paper)\n","\n","    return blank_paper\n","\n","\n","\n","# desired image size\n","image_shape = (512, 512, 3)\n","\n","\n","# Load and display texture image\n","image_C = tf.keras.preprocessing.image.load_img(\"/content/newyork.jfif\", target_size = image_shape)\n","\n","\"\"\"\n","Preprocess the input\n","\"\"\"\n","img_C = tf.keras.preprocessing.image.img_to_array(image_C) # convert image to array to feed into CNN\n","img_C = np.expand_dims(img_C, axis = 0) # we must have 1 dimension for the number of images\n","\n","img_C[0,:,:,:] = img_C[0,:,:,:] / 255.\n","\n","\n","plt.imshow(image_C)\n","plt.title(\"Sample Image\")\n","plt.show()\n","\n","plt.imshow(img_C[0,:,:,:])\n","plt.title(\"Content Image\")\n","plt.show()\n","\n","tf.keras.preprocessing.image.save_img(path = '/content/C.jpg', x = img_C[0,:,:,:])\n","\n","#########################################################################################################\n","\n","# Load and display perceptual image\n","image_P = tf.keras.preprocessing.image.load_img(\"/content/starrynight.jfif\", target_size = image_shape)\n","\n","\"\"\"\n","Preprocess the input\n","\"\"\"\n","img_P = tf.keras.preprocessing.image.img_to_array(image_P) # convert image to array to feed into CNN\n","img_P = np.expand_dims(img_P, axis = 0) # we must have 1 dimension for the number of images\n","\n","img_P[0,:,:,:] = img_P[0,:,:,:] / 255.\n","\n","\n","plt.imshow(image_P)\n","plt.title(\"Sample Image\")\n","plt.show()\n","\n","plt.imshow(img_P[0,:,:,:])\n","plt.title(\"Perceptual Image\")\n","plt.show()\n","\n","#########################################################################################################\n","\n","# activations of the Content Image in different layers in CNN, these activations are fixed\n","activation_C = model_activations(img_C, input = \"input_1\")\n","activation_P = model_activations(img_P, input = \"input_2\")\n","\n","# product initially is a blank paper\n","\n","# you should change this to True, if you start drawing in the first epoch\n","# I need several different epochs, each start with the past result to avoid GPU out of memory issue\n","blank = False \n","\n","if blank == True:\n","  product = create_template()\n","else:\n","  product = tf.keras.preprocessing.image.load_img(\"/content/C.jpg\", target_size = image_shape)\n","  product = tf.keras.preprocessing.image.img_to_array(product)\n","  product = product / 255.\n","  product = np.expand_dims(product, axis = 0)\n","  product = tf.Variable(product)\n","\n","\n","\n","plt.imshow(product[0,:,:,:])\n","plt.title(\"Start\")\n","plt.show()\n","\n","gamma_1 = 1e2 # how much do we care about the texture\n","gamma_2 = 1e2 # how much do we care about the perceptual\n","\n","\n","# Draw part\n","for i in range(0, 160):\n","    Draw(i, opt = tf.optimizers.Adam(learning_rate = 0.001))\n","\n","\n","\n","# Display the final product\n","prod = product.numpy()\n","\n","# save image for the next training; I have to do this due to limitation of GPU memory, else it will throw OOM error\n","tf.keras.preprocessing.image.save_img(path = '/content/C.jpg', x = prod[0,:,:,:])\n","\n","prod = tf.keras.preprocessing.image.array_to_img(prod[0,:,:,:])\n","\n","plt.imshow(prod)\n","plt.title(\"Final\")\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNgdiYCbvy9R7EfP7TMMQgX","collapsed_sections":[],"name":"StyleTransfer.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}